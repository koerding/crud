\section{Robustness checks}
\label{app:robustness}

The main text documents approximately power-law eigenvalue spectra across all
datasets and argues that the resulting broad spectral structure drives slow decay
of the background correlation scale under PC regression.
Two natural concerns arise: (i)~could the observed approximately power-law spectrum be an
artifact of heavy tails, outliers, or scaling properties of the raw data rather
than a genuine feature of the dependence structure? and (ii)~could the spectral
structure be consistent with independence among features, so that the observed
spectra are explainable by noise alone?
Both concerns can be addressed with systematic robustness checks, and in every
case the power-law structure survives.

\subsection{Heavy-tail stress test: is the power-law spectrum a preprocessing artifact?}
\label{app:heavy-tail}

Approximately $1/f$ structure could in principle arise from artifacts such as
outlier entries, heterogeneous row scales, or heavy-tailed marginal distributions,
rather than from genuine shared dependence among features.
If any of these were the dominant driver, then transformations that remove the
artifact should noticeably weaken or destroy the power-law behavior.
We test this by applying four transformations to each dataset before recomputing
the PCA spectrum:

\begin{enumerate}
\item \textbf{Clipping} at the 99.9th percentile: all entries beyond the
  0.1\%/99.9\% quantiles are clipped to the boundary values,
  removing extreme outliers while preserving the bulk of the distribution.
\item \textbf{Row-wise $L_2$ normalization}: each row (observation) is divided
  by its Euclidean norm, removing heterogeneous magnitude scaling across samples.
\item \textbf{Rank Gaussianization}: all entries are replaced by their
  inverse-normal-transformed ranks (via $z = \sqrt{2}\,\mathrm{erfinv}(2u-1)$
  where $u$ is the rank-based uniform quantile), completely removing marginal
  non-Gaussianity while preserving the rank-order dependence structure.
\item \textbf{Gaussian null}: an i.i.d.\ $N(0,1)$ matrix of matching dimensions
  (capped at $5000 \times 2000$) provides a baseline with no dependence structure.
\end{enumerate}

For each transformation, we fit the power law $\log(\lambda_k/\text{total var})
= a - \alpha\log k$ on the first 40 components (matching the procedure in
Section~\ref{app:assumption-audit}) and report the exponent~$\alpha$ and
goodness of fit~$R^2$.
We also compute the Hill tail index~$\hat\mu$ on the top 2\% of absolute entry
values, both on the raw data and after row normalization, to characterize
marginal tail heaviness.

\begin{table}[h]
\centering
\caption{Heavy-tail stress test: power-law fits ($\alpha$, $R^2$) on the first 40
principal components under different data transformations.
The Gaussian null (i.i.d.\ noise) produces a flat spectrum with no power-law structure.}
\label{tab:heavy-tail}
\small
\begin{tabular}{lcc cc cc cc}
\toprule
& \multicolumn{2}{c}{Original} & \multicolumn{2}{c}{Clip@0.999}
& \multicolumn{2}{c}{Row-norm} & \multicolumn{2}{c}{Rank-Gauss} \\
\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}
Dataset & $\alpha$ & $R^2$ & $\alpha$ & $R^2$ & $\alpha$ & $R^2$ & $\alpha$ & $R^2$ \\
\midrule
Kay fMRI    & 0.67 & 0.98 & 0.67 & 0.98 & 0.67 & 0.98 & 0.67 & 0.98 \\
Haxby fMRI  & 1.03 & 0.98 & 1.02 & 0.98 & 0.87 & 0.94 & 0.97 & 0.96 \\
NHANES      & 0.63 & 0.99 & 0.64 & 0.99 & 0.91 & 0.93 & 0.83 & 0.99 \\
CIFAR-10    & 1.24 & 1.00 & 1.24 & 1.00 & 1.09 & 0.99 & 1.25 & 1.00 \\
GTEx        & 1.30 & 0.99 & 1.30 & 0.99 & 1.24 & 0.99 & 1.30 & 0.99 \\
Precinct    & 0.70 & 0.99 & 0.70 & 0.99 & 0.76 & 0.97 & 0.77 & 0.98 \\
RNA-Seq     & 1.33 & 0.93 & 1.33 & 0.93 & 1.02 & 0.94 & 1.44 & 0.93 \\
HEXACO      & 0.97 & 0.97 & 0.97 & 0.97 & 0.94 & 0.97 & 1.02 & 0.97 \\
Stringer    & 0.74 & 0.99 & 0.74 & 0.99 & 0.72 & 0.99 & 0.89 & 0.99 \\
\bottomrule
\end{tabular}
\end{table}

The results (Table~\ref{tab:heavy-tail}) show that the power-law spectral
structure is robust to all transformations.
Clipping at the 99.9th percentile has essentially no effect on either $\alpha$
or $R^2$ in any dataset, ruling out extreme outlier entries as the driver.
Row normalization and rank Gaussianization produce modest shifts in $\alpha$
(typically $< 0.15$) but preserve the qualitative power-law shape with high
$R^2$ ($> 0.93$ in all cases).
The most notable change is that rank Gaussianization can slightly increase
$\alpha$ (e.g., RNA-Seq: $1.33 \to 1.44$; Stringer: $0.74 \to 0.89$),
suggesting that heavy tails in marginals, if anything, slightly flatten the
spectrum rather than create the power law.
By contrast, the Gaussian null (i.i.d.\ noise of matching dimensions) produces a
flat spectrum with no power-law structure, confirming that the observed spectral
decay reflects genuine shared dependence rather than a finite-sample artifact.

These checks support treating the broad, approximately $1/f$ eigenvalue spectrum
as a real structural property of these datasets.
The power-law behavior persists whether or not entries are clipped, rows are
rescaled, or marginals are Gaussianized, and it is absent in data with no
dependence structure.

\subsection{Null-model comparisons: are the spectra consistent with independence?}
\label{app:null-models}

A complementary question is whether the observed eigenvalue spectra could arise
from independent features.
If features were truly independent (or had only marginal structure but no
cross-feature dependence), then the PCA spectrum should resemble that of a noise
matrix.
We compare the empirical spectrum of each dataset against two null models:

\begin{enumerate}
\item \textbf{Gaussian null}: an i.i.d.\ $N(0,1)$ matrix of matching dimensions
  ($n \times p$, capped at $5000 \times 2000$).
  Under the Marchenko--Pastur law \citep{marchenko1967}, eigenvalues cluster in a bounded interval
  with no power-law tail.
\item \textbf{Column-permutation null}: the values within each column are
  independently shuffled across rows, destroying all cross-feature correlations
  while preserving each feature's marginal distribution exactly.
  This is a stricter control than the Gaussian null because it retains marginal
  non-Gaussianity, sparsity, and any column-specific scaling.
  We average over 3 independent permutation realizations.
\end{enumerate}

For each null model, we compute both the explained-variance spectrum and the
inverse participation ratio (IPR$_k = \sum_i v_{ik}^4$) of each eigenvector,
which measures eigenvector localization.

\paragraph{Results.}
The key question is whether the observed spectra could arise from independent features.
Across all nine datasets, the empirical eigenvalue spectrum separates
dramatically from both null models.
The empirical spectra show power-law decay spanning 1--2 orders of magnitude
above the null baseline, while both the Gaussian and column-permutation nulls
produce flat spectra clustered near $1/p$.
The separation is evident from the first principal component onward and persists
across the full range of computed components (up to 300).
The broad spectral structure reflects genuine cross-feature
dependence rather than marginal properties of individual features.

The column-permutation null is particularly informative: it preserves each
feature's exact marginal distribution (including heavy tails, discreteness, and
sparsity) but destroys all pairwise and higher-order dependence.
The fact that the column-permutation spectrum is indistinguishable from the
Gaussian null confirms that the observed power-law structure arises from
cross-feature dependence, not from marginal distributional properties.

Eigenvector localization provides a complementary diagnostic.
Empirical eigenvectors show moderately elevated IPR values compared to the null
models, particularly for the leading components, consistent with the partial
eigenvector localization documented in
Section~\ref{app:assumption-audit} (Test~2).
However, the IPR elevation is modest (typically 1.5--3$\times$ the null level)
and diminishes for higher-rank components.
Eigenvectors are not perfectly delocalized, but their localization is far from
extreme enough to invalidate the qualitative conclusions.

\subsection{Summary}

The robustness checks confirm two key properties of the datasets studied:
\begin{itemize}
\item The approximately power-law eigenvalue spectrum is not an artifact of
  outliers, row-scale heterogeneity, or marginal non-Gaussianity.
  It persists under clipping, row normalization, and rank Gaussianization,
  and it is absent in independence-preserving null models
  (Tables~\ref{tab:heavy-tail}).
\item The spectral structure reflects genuine cross-feature dependence:
  column permutation (which preserves marginals but destroys dependence)
  eliminates the power-law decay entirely.
\end{itemize}
Together with the assumption audit in Section~\ref{app:assumption-audit}, these
checks support the main text's interpretation: the broad spectral structure that
drives slow decay of the crud scale under PC regression is a real feature of
these datasets' dependence structure, not a preprocessing or distributional
artifact.

\subsection{Alternative adjustment methods: the problem is the spectrum, not PCA}
\label{app:alt-adjust}

The main text uses top-$K$ PC regression as a deliberately generic adjustment
strategy. A natural concern is whether the persistence of broad residual
correlations is specific to PCA, or whether it reflects a more fundamental
property of the datasets' dependence structure that would limit any
moderate-dimensional linear adjustment. We test this by repeating the core
analysis---adjust all features, recompute pairwise correlations on residuals,
measure the spread---under three alternative adjustment strategies that are
commonly used or proposed in observational causal inference practice.

\paragraph{Methods.}
\begin{enumerate}
\item \textbf{Random covariate regression.}
  For each feature, regress it on a randomly chosen set of $m$ other features
  (via OLS) and take the residual. This mimics the common practice of
  ``controlling for observables'' when no principled adjustment set is available.
  We average over 5 independent random covariate draws.

\item \textbf{Random projection adjustment.}
  Project the data onto $K$ random Gaussian directions (orthonormalized), regress
  those out, and measure residual correlation spread. This is a useful control
  because it tests whether the \emph{choice} of directions matters: if random
  projections give similar results to PCA, the problem is the dimensionality of
  shared variation, not the specific directions removed. We average over 5
  independent random projection draws.

\item \textbf{Lasso adjustment.}
  For each feature in the correlation subset, use Lasso ($\ell_1$-penalized
  regression, $\alpha = 0.1$) to data-adaptively select predictors from all other
  features, then compute pairwise correlations on the residuals. This tests
  whether data-driven variable selection can isolate the ``right'' confounders
  and collapse the background dependence. It maps onto doubly-robust and
  post-double-selection workflows popular in econometrics \citep{belloni2014}.
\end{enumerate}

For comparability, we measure the same quantity across all methods: the standard
deviation of off-diagonal residual correlations on a fixed random subset of
300 features (or all features if $p < 300$), at adjustment dimensions
$m \in \{10, 50, 100\}$ for the linear methods.

\begin{table}[h]
\centering
\caption{Residual correlation SD under alternative adjustment strategies.
PCA targets the top eigenvalues; random covariates mimic observational
practice; random projections remove generic directions; Lasso uses data-driven
variable selection. All methods leave substantial residual spread.}
\label{tab:alt-adjust}
\small
\begin{tabular}{l ccc ccc c}
\toprule
& \multicolumn{3}{c}{Dimension 10} & \multicolumn{3}{c}{Dimension 50} & \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
Dataset & PCA & Rand.\ cov. & Rand.\ proj. & PCA & Rand.\ cov. & Rand.\ proj. & Lasso \\
\midrule
Kay fMRI    & 0.036 & 0.059 & 0.065 & 0.027 & 0.044 & 0.065 & 0.032 \\
Stringer    & 0.019 & 0.028 & 0.030 & 0.015 & 0.024 & 0.030 & 0.017 \\
HEXACO      & 0.046 & 0.099 & 0.145 & 0.040 & 0.076 & 0.146 & 0.033 \\
Haxby fMRI  & 0.079 & 0.129 & 0.177 & 0.036 & 0.075 & 0.176 & 0.042 \\
RNA-Seq     & 0.071 & 0.125 & 0.182 & 0.043 & 0.067 & 0.227 & 0.037 \\
GTEx        & 0.101 & 0.170 & 0.285 & 0.056 & 0.095 & 0.284 & 0.063 \\
CIFAR-10    & 0.104 & 0.139 & 0.190 & 0.062 & 0.090 & 0.193 & 0.084 \\
NHANES      & 0.093 & 0.097 & 0.119 & 0.116 & 0.105 & 0.127 & 0.077 \\
Precinct    & 0.136 & 0.135 & 0.169 & 0.309$^\dagger$ & 0.135 & 0.207 & 0.094 \\
\bottomrule
\multicolumn{8}{l}{\footnotesize $^\dagger$Precinct has only $p{=}83$ features.
Removing 50 of 83 PCs leaves a near-degenerate residual subspace,}\\
\multicolumn{8}{l}{\footnotesize inflating the residual correlation SD.
This is a finite-$p$ artifact, not a property of the adjustment method.}
\end{tabular}
\end{table}

\paragraph{Results (Table~\ref{tab:alt-adjust}).}
The qualitative picture is the same across all methods---all leave substantial
residual spread---but the quantitative differences are real and informative.

\begin{enumerate}
\item \textbf{All methods leave substantial residual spread.}
  At dimension 10, the residual correlation SD ranges from 0.019 to 0.285
  depending on dataset and method; no method collapses it to near zero.
  Even Lasso, which can draw on all $p-1$ remaining features as potential
  predictors (not just $K$ directions), produces residual SDs of 0.017--0.094.

\item \textbf{PCA is the most efficient per dimension; random covariates are
  1.5--3$\times$ worse.}
  PCA targets the top eigenvalues by construction, so it removes the most
  shared variance per dimension. Random covariate regression---the procedure
  closest to common observational practice---produces residual SDs that are
  typically 1.5--2.2$\times$ larger than PCA at the same dimension
  (e.g., Haxby at dimension 10: PCA 0.079 vs.\ random covariates 0.129;
  HEXACO: 0.046 vs.\ 0.099).
  This gap reflects the fact that randomly chosen covariates are unlikely to
  align with the dominant shared directions, so they remove less dependence
  structure per covariate.

\item \textbf{Random projections are nearly inert for high-$p$ datasets.}
  For CIFAR-10 ($p{=}3072$), Haxby ($p{=}1452$), and Stringer ($p{=}6000$),
  removing 10, 50, or 100 random directions produces virtually identical
  residual SDs (e.g., CIFAR-10: 0.190, 0.193, 0.194).
  This is the cleanest evidence for the spectral explanation: when the
  eigenvalue spectrum is broad, random directions miss the shared variance
  entirely, so projecting them out has negligible effect on the correlation
  structure.

\item \textbf{Lasso can outperform PCA but still cannot collapse the background.}
  Lasso uses data-driven variable selection from all other features, giving it
  access to more effective predictors than a fixed $K$-dimensional subspace.
  It often produces lower residual SDs than PCA at comparable effective
  dimensionality (e.g., Haxby: Lasso 0.042 vs.\ PCA at $K{=}10$: 0.079;
  Precinct: 0.094 vs.\ 0.136). Nevertheless, even Lasso leaves residual SDs
  of 0.017--0.094 across datasets, confirming that the background dependence
  cannot be eliminated by any single-step linear adjustment.
\end{enumerate}

The methods differ by factors of roughly 1.5--3$\times$, which is not
negligible---but the central point is that none of them collapse the residual
correlation spread to near zero. Whether one uses PCA, random covariates, Lasso,
or random projections, residual correlation SDs remain in the range
0.02--0.28. The crud scale is a property of the data's spectral structure,
not of the particular adjustment method.

\subsection{Additional worked examples}
\label{app:additional-examples}

\paragraph{NHANES biomarkers.} Hemoglobin and hematocrit are known to be near-deterministically related (hematocrit $\approx 3\times$ hemoglobin by definition). In the NHANES dataset ($n = 29{,}902$), their correlation is $r = 0.91$. Applying the crud-aware $z$-test with $\sigma_{\mathrm{crud}} = 0.093$ (NHANES at $K=10$) gives $z_{\mathrm{crud}} = 9.8$, $p_{\mathrm{crud}} < 10^{-20}$. Similarly, ALT and AST (both liver transaminases reflecting hepatocyte damage) correlate at $r = 0.83$, giving $z_{\mathrm{crud}} = 8.9$. Both associations survive the crud-aware calibration by a wide margin, as expected for pairs with known mechanistic relationships.

\paragraph{GTEx RNA-seq.} Most cis-eQTLs reported in the GTEx v8 atlas have small effect sizes, with only about 22\% showing $>$2-fold effects \citep{gtex2020}. Using the GTEx v8 per-sample TPM data for skeletal muscle (top 10,000 genes by variance across 803 samples), the crud distribution at $K=10$ has a background SD of 0.10. A typical small cis-eQTL effect of $r=0.10$ sits at only the 71st crud percentile---squarely within the bulk of background gene--gene correlations. Even $r=0.20$ reaches only the 95th percentile. Only effects with $r>0.30$ (the 99th percentile, $3\times$ the background SD) are clearly distinctive. Since most reported cis-eQTLs have small effect sizes, much of the GTEx catalog may not be distinguishable from background transcriptomic dependence using association magnitude alone.

\subsection{Positive control: known-strong pairs survive crud-aware calibration}
\label{app:positive-control}

The crud-aware calibration framework (Section~\ref{app:crud-aware-pvalues}) is designed to
flag associations that stand out from the background. A natural concern is whether genuinely
strong associations---pairs with known biological or psychometric relationships---do in fact
survive this calibration. If the crud-aware null swallowed real signal, it would be too
conservative to be useful.

We test this using two datasets where ground-truth group structure is available:

\begin{itemize}
\item \textbf{HEXACO personality inventory} ($p{=}242$): items within the same personality
  facet (e.g., the 10 Honesty-Sincerity items) are designed by psychometricians to measure the
  same underlying trait. There are 24 facets yielding 1080 within-facet pairs.
\item \textbf{NHANES biomedical survey} ($p{=}165$): features within the same clinical
  subsystem (e.g., lipid panel: total cholesterol, HDL, triglycerides, LDL; or hematology:
  RBC, hemoglobin, hematocrit, MCV) are known to be biologically linked.
  Seven biomarker groups yield 54 within-group pairs.
\end{itemize}

For each dataset, we compute the residual correlation matrix after removing $K$ PCs, then
compute the crud-aware p-value for each known-strong pair: the fraction of all
$\binom{p}{2}$ pairs with $|\hat\rho^{(K)}| \ge |\hat\rho^{(K)}_{ij}|$.
If the positive control works, known-strong pairs should be heavily enriched in the
extreme tails.

\begin{table}[h]
\centering
\caption{Positive control: known-strong pairs are enriched in the tails of the crud-aware
null. ``Top 5\%'' and ``Top 1\%'' report the fraction of known-strong pairs with
crud-aware p-value below 0.05 and 0.01, respectively (expected: 5\% and 1\% for
random pairs).}
\label{tab:positive-control}
\begin{tabular}{llccccc}
\toprule
Dataset & $K$ & Median $|r|$ & Median $p_{\mathrm{crud}}$ & Top 5\% & Top 1\% & BG SD \\
\midrule
HEXACO  & 0  & 0.324 & 0.041 & 55\% & 20\% & 0.145 \\
HEXACO  & 10 & 0.044 & 0.254 & 29\% & 15\% & 0.046 \\
HEXACO  & 20 & 0.048 & 0.148 & 33\% & 14\% & 0.038 \\
\midrule
NHANES  & 0  & 0.389 & 0.016 & 69\% & 41\% & 0.115 \\
NHANES  & 10 & 0.131 & 0.085 & 44\% & 30\% & 0.093 \\
NHANES  & 20 & 0.390 & 0.010 & 70\% & 54\% & 0.099 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Results (Table~\ref{tab:positive-control}).}
The central question is whether the crud-aware null is too aggressive---whether it
swallows real signal along with background noise.
Known-strong pairs are massively enriched in the tails of the crud-aware null at every
level of adjustment, confirming that it does not.

In HEXACO, 55\% of within-facet pairs fall in the top 5\% at $K{=}0$, and 20\% fall in
the top 1\%---enrichments of $11\times$ and $20\times$ over chance. After removing 10 PCs,
29\% remain in the top 5\% and 15\% in the top 1\% ($6\times$ and $15\times$ enrichment).
After 20 PCs, enrichment is similar. The median within-facet $|r|$ drops from 0.32 to
0.04--0.05 after adjustment (comparable to the background SD of 0.04--0.05), yet the
enrichment persists because within-facet correlations are systematically larger than
typical random-pair correlations at the same adjustment level.
Psychometrically designed pairs survive crud-aware calibration at every adjustment level.

In NHANES, the enrichment is even stronger: 69\% of biomarker-group pairs are in the
top 5\% at $K{=}0$, rising to 70\% at $K{=}20$ (where 54\% are in the top 1\%). This
reflects the strong mechanistic links within clinical subsystems (e.g., hemoglobin and
hematocrit are physiologically coupled, systolic and diastolic blood pressure are
mechanically linked).
Biologically grounded pairs are clearly distinctive against the domain background.

The upshot is that the crud-aware calibration framework separates signal from noise
in the right direction: genuinely strong associations survive calibration and are clearly
distinguished from the background, even after generic PC adjustment.
At the same time, the bulk of random-pair associations remain near the crud scale,
reinforcing the main text's conclusion that most observed associations in these domains
reflect background dependence rather than specific causal links.

